willem@willem-Latitude-5590:~$ ping -c3 willem-Latitude-5590
PING willem-Latitude-5590.kpn (192.168.2.4) 56(84) bytes of data.
64 bytes from willem-Latitude-5590 (192.168.2.4): icmp_seq=1 ttl=64 time=0.052 ms
64 bytes from willem-Latitude-5590 (192.168.2.4): icmp_seq=2 ttl=64 time=0.068 ms
64 bytes from willem-Latitude-5590 (192.168.2.4): icmp_seq=3 ttl=64 time=0.114 ms

--- willem-Latitude-5590.kpn ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2025ms
rtt min/avg/max/mdev = 0.052/0.078/0.114/0.026 ms
willem@willem-Latitude-5590:~$ traceroute willem-Latitude-5590
traceroute to willem-Latitude-5590.kpn (192.168.2.4), 64 hops max
  1   192.168.2.4  0,007ms  0,004ms  0,003ms 
willem@willem-Latitude-5590:~$ hostnamectl
   Static hostname: willem-Latitude-5590
         Icon name: computer-laptop
           Chassis: laptop
        Machine ID: 82929df7ee394b73b81252fe3b4e5020
           Boot ID: 2ecfc8fa942249c6b68a3b7263818415
  Operating System: Linux Mint 19.3
            Kernel: Linux 5.4.0-80-generic
      Architecture: x86-64
willem@willem-Latitude-5590:~$ cd App/spark-3.1.1-bin-hadoop2.7/sbin
willem@willem-Latitude-5590:~/App/spark-3.1.1-bin-hadoop2.7/sbin$ ./start-master.sh 
Manual fix to force stand-alone on willem-Latitude-5590
SPARK_MASTER_HOST is set to  willem-Latitude-5590
starting org.apache.spark.deploy.master.Master, logging to /home/willem/App/spark-3.1.1-bin-hadoop2.7/logs/spark-willem-org.apache.spark.deploy.master.Master-1-willem-Latitude-5590.out
willem@willem-Latitude-5590:~/App/spark-3.1.1-bin-hadoop2.7/sbin$ ./start-worker.sh spark://willem-Latitude-5590:7077
starting org.apache.spark.deploy.worker.Worker, logging to /home/willem/App/spark-3.1.1-bin-hadoop2.7/logs/spark-willem-org.apache.spark.deploy.worker.Worker-1-willem-Latitude-5590.out
willem@willem-Latitude-5590:~/App/spark-3.1.1-bin-hadoop2.7/sbin$ cd ../bin
willem@willem-Latitude-5590:~/App/spark-3.1.1-bin-hadoop2.7/bin$ ./spark-submit --class net.jgp.books.spark.ch05.lab210_pi_compute_cluster_submit_job.PiComputeClusterSubmitJobApp --master "spark://willem-Latitude-5590:7077" /home/willem/resources/git/net.jgp.books.spark.ch05/target/spark-in-action2-chapter05-1.0.0-SNAPSHOT-uber.jar
21/07/30 18:37:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
About to throw 100000 darts, ready? Stay away from the target!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/07/30 18:37:38 INFO SparkContext: Running Spark version 3.1.1
21/07/30 18:37:38 INFO ResourceUtils: ==============================================================
21/07/30 18:37:38 INFO ResourceUtils: No custom resources configured for spark.driver.
21/07/30 18:37:38 INFO ResourceUtils: ==============================================================
21/07/30 18:37:38 INFO SparkContext: Submitted application: JavaSparkPi on a cluster (via spark-submit)
21/07/30 18:37:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/07/30 18:37:38 INFO ResourceProfile: Limiting resource is cpu
21/07/30 18:37:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/07/30 18:37:39 INFO SecurityManager: Changing view acls to: willem
21/07/30 18:37:39 INFO SecurityManager: Changing modify acls to: willem
21/07/30 18:37:39 INFO SecurityManager: Changing view acls groups to: 
21/07/30 18:37:39 INFO SecurityManager: Changing modify acls groups to: 
21/07/30 18:37:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(willem); groups with view permissions: Set(); users  with modify permissions: Set(willem); groups with modify permissions: Set()
21/07/30 18:37:39 INFO Utils: Successfully started service 'sparkDriver' on port 33415.
21/07/30 18:37:39 INFO SparkEnv: Registering MapOutputTracker
21/07/30 18:37:39 INFO SparkEnv: Registering BlockManagerMaster
21/07/30 18:37:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/30 18:37:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/30 18:37:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/30 18:37:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57ecdc31-19b9-45ba-94fc-0bc7bbd9f1b8
21/07/30 18:37:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/07/30 18:37:39 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/30 18:37:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/30 18:37:39 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4040
21/07/30 18:37:39 INFO SparkContext: Added JAR file:/home/willem/resources/git/net.jgp.books.spark.ch05/target/spark-in-action2-chapter05-1.0.0-SNAPSHOT-uber.jar at spark://localhost:33415/jars/spark-in-action2-chapter05-1.0.0-SNAPSHOT-uber.jar with timestamp 1627663058853
21/07/30 18:37:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://willem-Latitude-5590:7077...
21/07/30 18:37:40 INFO TransportClientFactory: Successfully created connection to willem-Latitude-5590/192.168.2.4:7077 after 35 ms (0 ms spent in bootstraps)
21/07/30 18:37:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210730183740-0000
21/07/30 18:37:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45841.
21/07/30 18:37:40 INFO NettyBlockTransferService: Server created on localhost:45841
21/07/30 18:37:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/30 18:37:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 45841, None)
21/07/30 18:37:40 INFO BlockManagerMasterEndpoint: Registering block manager localhost:45841 with 366.3 MiB RAM, BlockManagerId(driver, localhost, 45841, None)
21/07/30 18:37:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 45841, None)
21/07/30 18:37:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 45841, None)
21/07/30 18:37:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210730183740-0000/0 on worker-20210730183624-127.0.0.1-34711 (127.0.0.1:34711) with 8 core(s)
21/07/30 18:37:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20210730183740-0000/0 on hostPort 127.0.0.1:34711 with 8 core(s), 4.0 GiB RAM
21/07/30 18:37:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210730183740-0000/0 is now RUNNING
21/07/30 18:37:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
Session initialized in 2058 ms
21/07/30 18:37:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/willem/App/spark-3.1.1-bin-hadoop2.7/bin/spark-warehouse').
21/07/30 18:37:42 INFO SharedState: Warehouse path is 'file:/home/willem/App/spark-3.1.1-bin-hadoop2.7/bin/spark-warehouse'.
21/07/30 18:37:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:58424) with ID 0,  ResourceProfileId 0
21/07/30 18:37:43 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:46773 with 2004.6 MiB RAM, BlockManagerId(0, 127.0.0.1, 46773, None)
21/07/30 18:37:44 INFO CodeGenerator: Code generated in 191.855678 ms
Initial dataframe built in 4239 ms
Throwing darts done in 99 ms
21/07/30 18:37:46 INFO CodeGenerator: Code generated in 9.725757 ms
21/07/30 18:37:47 INFO SparkContext: Starting job: reduce at PiComputeClusterSubmitJobApp.java:105
21/07/30 18:37:47 INFO DAGScheduler: Got job 0 (reduce at PiComputeClusterSubmitJobApp.java:105) with 8 output partitions
21/07/30 18:37:47 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at PiComputeClusterSubmitJobApp.java:105)
21/07/30 18:37:47 INFO DAGScheduler: Parents of final stage: List()
21/07/30 18:37:47 INFO DAGScheduler: Missing parents: List()
21/07/30 18:37:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at reduce at PiComputeClusterSubmitJobApp.java:105), which has no missing parents
21/07/30 18:37:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.3 KiB, free 366.3 MiB)
21/07/30 18:37:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KiB, free 366.3 MiB)
21/07/30 18:37:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:45841 (size: 4.6 KiB, free: 366.3 MiB)
21/07/30 18:37:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
21/07/30 18:37:47 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at reduce at PiComputeClusterSubmitJobApp.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
21/07/30 18:37:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks resource profile 0
21/07/30 18:37:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (127.0.0.1, executor 0, partition 0, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (127.0.0.1, executor 0, partition 1, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (127.0.0.1, executor 0, partition 2, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (127.0.0.1, executor 0, partition 3, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (127.0.0.1, executor 0, partition 4, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (127.0.0.1, executor 0, partition 5, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (127.0.0.1, executor 0, partition 6, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:47 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (127.0.0.1, executor 0, partition 7, PROCESS_LOCAL, 417124 bytes) taskResourceAssignments Map()
21/07/30 18:37:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:46773 (size: 4.6 KiB, free: 2004.6 MiB)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2695 ms on 127.0.0.1 (executor 0) (1/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2614 ms on 127.0.0.1 (executor 0) (2/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2629 ms on 127.0.0.1 (executor 0) (3/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2595 ms on 127.0.0.1 (executor 0) (4/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2642 ms on 127.0.0.1 (executor 0) (5/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2584 ms on 127.0.0.1 (executor 0) (6/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2661 ms on 127.0.0.1 (executor 0) (7/8)
21/07/30 18:37:50 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2606 ms on 127.0.0.1 (executor 0) (8/8)
21/07/30 18:37:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/07/30 18:37:50 INFO DAGScheduler: ResultStage 0 (reduce at PiComputeClusterSubmitJobApp.java:105) finished in 2.944 s
21/07/30 18:37:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/07/30 18:37:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/07/30 18:37:50 INFO DAGScheduler: Job 0 finished: reduce at PiComputeClusterSubmitJobApp.java:105, took 3.006480 s
Analyzing result in 4883 ms
Pi is roughly 3.1446
21/07/30 18:37:50 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
21/07/30 18:37:50 INFO StandaloneSchedulerBackend: Shutting down all executors
21/07/30 18:37:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
21/07/30 18:37:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/07/30 18:37:50 INFO MemoryStore: MemoryStore cleared
21/07/30 18:37:50 INFO BlockManager: BlockManager stopped
21/07/30 18:37:50 INFO BlockManagerMaster: BlockManagerMaster stopped
21/07/30 18:37:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/07/30 18:37:50 INFO SparkContext: Successfully stopped SparkContext
21/07/30 18:37:50 INFO ShutdownHookManager: Shutdown hook called
21/07/30 18:37:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-d9ace1b0-017a-4022-9e5b-fa8fc696478d
21/07/30 18:37:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-38eb5f0e-2d1a-43f7-a3a8-33d2e596ed0d
willem@willem-Latitude-5590:~/App/spark-3.1.1-bin-hadoop2.7/bin$ 
